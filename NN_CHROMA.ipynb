{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "class WavFileHelper():\n",
    "    \n",
    "    def read_file_properties(self, filename):\n",
    "\n",
    "        wave_file = open(filename,\"rb\")\n",
    "        \n",
    "        riff = wave_file.read(12)\n",
    "        fmt = wave_file.read(36)\n",
    "        \n",
    "        num_channels_string = fmt[10:12]\n",
    "        num_channels = struct.unpack('<H', num_channels_string)[0]\n",
    "\n",
    "        sample_rate_string = fmt[12:16]\n",
    "        sample_rate = struct.unpack(\"<I\",sample_rate_string)[0]\n",
    "        \n",
    "        bit_depth_string = fmt[22:24]\n",
    "        bit_depth = struct.unpack(\"<H\",bit_depth_string)[0]\n",
    "\n",
    "        return (num_channels, sample_rate, bit_depth)\n",
    "wavfilehelper = WavFileHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audiodata = []\n",
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = 'G:/urban_dataset'\n",
    "\n",
    "metadata = pd.read_csv(fulldatasetpath + '/metadata/UrbanSound8K.csv')\n",
    "\n",
    "\n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath('G:/urban_dataset/'),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    data = wavfilehelper.read_file_properties(file_name)\n",
    "    audiodata.append(data)\n",
    "\n",
    "# Convert into a Panda dataframe\n",
    "audiodf = pd.DataFrame(audiodata, columns=['num_channels','sample_rate','bit_depth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') # default is Kaiser_best\n",
    "        stft = np.abs(librosa.stft(audio))\n",
    "        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file)\n",
    "        return None \n",
    "     \n",
    "    return chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\librosa\\core\\pitch.py:146: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn('Trying to estimate tuning from empty frequency set.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  8732  files\n"
     ]
    }
   ],
   "source": [
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = 'G:/urban_dataset'\n",
    "\n",
    "metadata = pd.read_csv(fulldatasetpath + '/metadata/UrbanSound8K.csv')\n",
    "\n",
    "features = []\n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    class_label = row[\"class\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network dimensions\n",
    "n_dim =x_train.shape[1]\n",
    "n_classes = y_train.shape[1]\n",
    "n_hidden_units_1 = n_dim\n",
    "n_hidden_units_2 = 400 # approx n_dim * 2\n",
    "n_hidden_units_3 = 200 # half of layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "Features: 12 Classes: 10 \n"
     ]
    }
   ],
   "source": [
    "print(n_dim)\n",
    "print(\"Features: {} Classes: {} \".format(n_dim, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(activation_function='relu',init_type='normal', kernel_initializer=\"uniform\", optimiser='Adamax', dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    # layer 1\n",
    "    #model.add(Convolution2D(n_hidden_units_1, input_dim=n_dim, activation=activation_function,kernel_initializer=\"uniform\"))\n",
    "    #model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dense(n_hidden_units_1, input_dim=n_dim, init=init_type, activation=activation_function))\n",
    "    # layer 2\n",
    "    model.add(Dense(n_hidden_units_2, kernel_initializer=\"uniform\", activation=activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # layer 3\n",
    "    model.add(Dense(n_hidden_units_3, kernel_initializer=\"uniform\" , activation=activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    # output layer\n",
    "    model.add(Dense(n_classes, kernel_initializer=\"uniform\", activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimiser, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, input_dim=12, activation=\"relu\", kernel_initializer=\"normal\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               5200      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 87,566\n",
      "Trainable params: 87,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6286 samples, validate on 699 samples\n",
      "Epoch 1/30\n",
      "6286/6286 [==============================] - 3s 441us/step - loss: 2.2569 - acc: 0.1102 - val_loss: 2.2282 - val_acc: 0.1359\n",
      "Epoch 2/30\n",
      "6286/6286 [==============================] - 2s 385us/step - loss: 2.1932 - acc: 0.1729 - val_loss: 2.0916 - val_acc: 0.2203\n",
      "Epoch 3/30\n",
      "6286/6286 [==============================] - 2s 346us/step - loss: 2.0535 - acc: 0.2014 - val_loss: 1.9799 - val_acc: 0.2017\n",
      "Epoch 4/30\n",
      "6286/6286 [==============================] - 2s 370us/step - loss: 2.0021 - acc: 0.2119 - val_loss: 1.9608 - val_acc: 0.2375\n",
      "Epoch 5/30\n",
      "6286/6286 [==============================] - 2s 368us/step - loss: 1.9744 - acc: 0.2171 - val_loss: 2.0046 - val_acc: 0.2031\n",
      "Epoch 6/30\n",
      "6286/6286 [==============================] - 2s 372us/step - loss: 1.9695 - acc: 0.2133 - val_loss: 1.9376 - val_acc: 0.2418\n",
      "Epoch 7/30\n",
      "6286/6286 [==============================] - 2s 387us/step - loss: 1.9565 - acc: 0.2211 - val_loss: 1.9241 - val_acc: 0.2589\n",
      "Epoch 8/30\n",
      "6286/6286 [==============================] - 3s 410us/step - loss: 1.9574 - acc: 0.2232 - val_loss: 1.9118 - val_acc: 0.2446\n",
      "Epoch 9/30\n",
      "6286/6286 [==============================] - 2s 394us/step - loss: 1.9349 - acc: 0.2281 - val_loss: 1.9085 - val_acc: 0.2518\n",
      "Epoch 10/30\n",
      "6286/6286 [==============================] - 2s 368us/step - loss: 1.9325 - acc: 0.2275 - val_loss: 1.9040 - val_acc: 0.2546\n",
      "Epoch 11/30\n",
      "6286/6286 [==============================] - 2s 372us/step - loss: 1.9282 - acc: 0.2294 - val_loss: 1.8974 - val_acc: 0.2475\n",
      "Epoch 12/30\n",
      "6286/6286 [==============================] - 2s 379us/step - loss: 1.9234 - acc: 0.2267 - val_loss: 1.8969 - val_acc: 0.2375\n",
      "Epoch 13/30\n",
      "6286/6286 [==============================] - 2s 376us/step - loss: 1.9135 - acc: 0.2234 - val_loss: 1.9001 - val_acc: 0.2418\n",
      "Epoch 14/30\n",
      "6286/6286 [==============================] - 3s 412us/step - loss: 1.8953 - acc: 0.2402 - val_loss: 1.8951 - val_acc: 0.2132\n",
      "Epoch 15/30\n",
      "6286/6286 [==============================] - 3s 407us/step - loss: 1.8843 - acc: 0.2574 - val_loss: 1.8626 - val_acc: 0.2675\n",
      "Epoch 16/30\n",
      "6286/6286 [==============================] - 3s 400us/step - loss: 1.8771 - acc: 0.2534 - val_loss: 1.8541 - val_acc: 0.2790\n",
      "Epoch 17/30\n",
      "6286/6286 [==============================] - 3s 423us/step - loss: 1.8659 - acc: 0.2714 - val_loss: 1.8564 - val_acc: 0.2818\n",
      "Epoch 18/30\n",
      "6286/6286 [==============================] - 3s 451us/step - loss: 1.8571 - acc: 0.2690 - val_loss: 1.8266 - val_acc: 0.2876\n",
      "Epoch 19/30\n",
      "6286/6286 [==============================] - 2s 361us/step - loss: 1.8431 - acc: 0.2752 - val_loss: 1.8200 - val_acc: 0.2876\n",
      "Epoch 20/30\n",
      "6286/6286 [==============================] - 2s 393us/step - loss: 1.8388 - acc: 0.2840 - val_loss: 1.8164 - val_acc: 0.2976\n",
      "Epoch 21/30\n",
      "6286/6286 [==============================] - 2s 378us/step - loss: 1.8301 - acc: 0.2846 - val_loss: 1.8106 - val_acc: 0.2804\n",
      "Epoch 22/30\n",
      "6286/6286 [==============================] - 3s 400us/step - loss: 1.8125 - acc: 0.2897 - val_loss: 1.8410 - val_acc: 0.2947\n",
      "Epoch 23/30\n",
      "6286/6286 [==============================] - 2s 395us/step - loss: 1.8255 - acc: 0.2849 - val_loss: 1.8072 - val_acc: 0.2933\n",
      "Epoch 24/30\n",
      "6286/6286 [==============================] - 2s 391us/step - loss: 1.8059 - acc: 0.2943 - val_loss: 1.8077 - val_acc: 0.2933\n",
      "Epoch 25/30\n",
      "6286/6286 [==============================] - 3s 401us/step - loss: 1.8051 - acc: 0.2891 - val_loss: 1.8010 - val_acc: 0.3047\n",
      "Epoch 26/30\n",
      "6286/6286 [==============================] - 2s 390us/step - loss: 1.7968 - acc: 0.2938 - val_loss: 1.7851 - val_acc: 0.3090\n",
      "Epoch 27/30\n",
      "6286/6286 [==============================] - 3s 408us/step - loss: 1.7797 - acc: 0.3016 - val_loss: 1.8008 - val_acc: 0.2918\n",
      "Epoch 28/30\n",
      "6286/6286 [==============================] - 3s 435us/step - loss: 1.7785 - acc: 0.3029 - val_loss: 1.7922 - val_acc: 0.2876\n",
      "Epoch 29/30\n",
      "6286/6286 [==============================] - 3s 410us/step - loss: 1.7708 - acc: 0.3067 - val_loss: 1.7780 - val_acc: 0.3033\n",
      "Epoch 30/30\n",
      "6286/6286 [==============================] - 3s 403us/step - loss: 1.7684 - acc: 0.3140 - val_loss: 1.7861 - val_acc: 0.3033\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=30, validation_split=0.1, callbacks=[earlystop], batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1747/1747 [==============================] - 0s 134us/step\n",
      "ACCURACY: 30.68116771779063\n",
      "SCORE: 1.7754048659723694\n"
     ]
    }
   ],
   "source": [
    "score, accuracy = model.evaluate(x = x_test, y = y_test , verbose = 1)\n",
    "print(\"ACCURACY:\",(accuracy*100))\n",
    "print(\"SCORE:\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model.save(\"nn_chroma_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               5200      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 87,566\n",
      "Trainable params: 87,566\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "acc: 30.68%\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model\n",
    " \n",
    "# load model\n",
    "model = load_model('nn_chroma_model.h5')\n",
    "# summarize model.\n",
    "model.summary()\n",
    "# load dataset\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(x=x_test,y=y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6(tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
